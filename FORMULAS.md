# ðŸ“ Formules MathÃ©matiques - RÃ©fÃ©rence Rapide

## Self-Attention

### Scaled Dot-Product Attention
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

**OÃ¹:**
- Q (Query): matrice de requÃªtes (n Ã— d_k)
- K (Key): matrice de clÃ©s (n Ã— d_k)
- V (Value): matrice de valeurs (n Ã— d_v)
- d_k: dimension des clÃ©s
- n: longueur de sÃ©quence

### Ã‰tapes de calcul:
1. **Scores**: `S = QK^T` â†’ (n Ã— n)
2. **Scaling**: `S_scaled = S / âˆšd_k`
3. **Softmax**: `A = softmax(S_scaled)` â†’ poids d'attention
4. **Output**: `O = AV` â†’ (n Ã— d_v)

### Pourquoi âˆšd_k?
- Variance de QK^T â‰ˆ d_k
- Division par âˆšd_k normalise Ã  variance â‰ˆ 1
- Ã‰vite les valeurs extrÃªmes dans le softmax

---

## Multi-Head Attention

### Formule gÃ©nÃ©rale:
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O

oÃ¹ head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)
```

**ParamÃ¨tres:**
- h: nombre de tÃªtes
- d_model: dimension du modÃ¨le
- d_k = d_v = d_model / h

**Matrices de projection:**
- W^Q_i âˆˆ â„^(d_model Ã— d_k)
- W^K_i âˆˆ â„^(d_model Ã— d_k)
- W^V_i âˆˆ â„^(d_model Ã— d_v)
- W^O âˆˆ â„^(hd_v Ã— d_model)

### ComplexitÃ©:
- **Temps**: O(nÂ² Â· d_model)
- **Espace**: O(nÂ² + n Â· d_model)

---

## Positional Encoding

### Formule sinusoÃ¯dale:
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**OÃ¹:**
- pos: position dans la sÃ©quence (0 Ã  n-1)
- i: dimension (0 Ã  d_model/2 - 1)
- 2i: indices pairs
- 2i+1: indices impairs

### PropriÃ©tÃ©s:
1. **Unique**: Chaque position a un encodage distinct
2. **BornÃ©**: Valeurs entre -1 et 1
3. **Distance relative**: PE(pos+k) est une fonction linÃ©aire de PE(pos)

### FrÃ©quences:
```
Î»_i = 10000^(2i/d_model)
```
- Dimension basse (i=0): Î» = 1, haute frÃ©quence
- Dimension haute (i=d_model/2): Î» = 10000, basse frÃ©quence

---

## Layer Normalization

### Formule:
```
LayerNorm(x) = Î³ âŠ™ (x - Î¼) / Ïƒ + Î²
```

**OÃ¹:**
- Î¼ = mean(x): moyenne sur la dimension d_model
- Ïƒ = std(x): Ã©cart-type sur la dimension d_model
- Î³: paramÃ¨tres d'Ã©chelle (appris)
- Î²: paramÃ¨tres de dÃ©calage (appris)
- âŠ™: multiplication Ã©lÃ©ment par Ã©lÃ©ment

### Calcul:
```
Î¼ = (1/d_model) Î£ x_i
ÏƒÂ² = (1/d_model) Î£ (x_i - Î¼)Â²
Ïƒ = âˆš(ÏƒÂ² + Îµ)  # Îµ pour stabilitÃ© numÃ©rique
```

---

## Feed-Forward Network

### Formule:
```
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
```

**Dimensions:**
- x: (batch, seq_len, d_model)
- W_1: (d_model, d_ff)
- W_2: (d_ff, d_model)
- d_ff typiquement = 4 Ã— d_model

### Ã‰quivalent fonctionnel:
```
FFN(x) = ReLU(Linear_1(x))
output = Linear_2(FFN)
```

---

## Transformer Encoder Block

### Architecture complÃ¨te:
```
Block(x):
  # Sub-layer 1: Multi-Head Attention
  x_1 = MultiHeadAttention(x, x, x)
  x = LayerNorm(x + Dropout(x_1))
  
  # Sub-layer 2: Feed-Forward
  x_2 = FFN(x)
  x = LayerNorm(x + Dropout(x_2))
  
  return x
```

### Avec notation mathÃ©matique:
```
xÌ‚ = LayerNorm(x + MultiHeadAttention(x))
output = LayerNorm(xÌ‚ + FFN(xÌ‚))
```

---

## Masques d'Attention

### Padding Mask:
```
M_pad[i, j] = {
  -âˆž  si token_j est un PAD
  0   sinon
}
```

### Causal Mask (Look-ahead):
```
M_causal[i, j] = {
  -âˆž  si j > i  (futur)
  0   si j â‰¤ i  (passÃ©/prÃ©sent)
}
```

### Application:
```
Attention = softmax((QK^T / âˆšd_k) + Mask)V
```

---

## Softmax

### Formule:
```
softmax(x_i) = exp(x_i) / Î£_j exp(x_j)
```

### Stable numÃ©riquement:
```
softmax(x_i) = exp(x_i - max(x)) / Î£_j exp(x_j - max(x))
```

**PropriÃ©tÃ©s:**
- Sortie entre 0 et 1
- Î£ softmax(x_i) = 1
- DiffÃ©rentiable

---

## ComplexitÃ© Totale

### Par couche Transformer:
- **Multi-Head Attention**: O(nÂ² Â· d_model + n Â· d_modelÂ²)
- **Feed-Forward**: O(n Â· d_model Â· d_ff)
- **Total**: O(nÂ² Â· d_model + n Â· d_model Â· d_ff)

### Pour L couches:
- **Temps**: O(L Â· (nÂ² Â· d_model + n Â· d_model Â· d_ff))
- **MÃ©moire**: O(n Â· d_model + L Â· nÂ²) pour stockage attention

### Avec paramÃ¨tres typiques (BERT-base):
- L = 12
- d_model = 768
- d_ff = 3072
- ParamÃ¨tres totaux â‰ˆ 110M

---

## Fonctions d'Activation

### ReLU:
```
ReLU(x) = max(0, x)
```

### GELU (souvent utilisÃ©):
```
GELU(x) â‰ˆ x Â· Î¦(x)
oÃ¹ Î¦(x) = (1/2)[1 + erf(x/âˆš2)]
```

### Approximation GELU:
```
GELU(x) â‰ˆ 0.5x(1 + tanh[âˆš(2/Ï€)(x + 0.044715xÂ³)])
```

---

## Initialisation des Poids

### Xavier/Glorot:
```
W ~ U(-âˆš(6/(n_in + n_out)), âˆš(6/(n_in + n_out)))
```

### He/Kaiming (pour ReLU):
```
W ~ N(0, âˆš(2/n_in))
```

---

## Gradient Flow

### Residual Connection:
```
y = x + F(x)
âˆ‚y/âˆ‚x = 1 + âˆ‚F(x)/âˆ‚x
```
- Le gradient peut toujours passer (terme 1)
- Ã‰vite le problÃ¨me de vanishing gradient

### Layer Norm Gradient:
```
âˆ‚LayerNorm/âˆ‚x = Î³/Ïƒ Â· (I - (1/d)(1Â·1^T + (x-Î¼)(x-Î¼)^T/ÏƒÂ²))
```

---

## Learning Rate Schedules

### Warmup puis Decay:
```
lr(t) = d_model^(-0.5) Â· min(t^(-0.5), t Â· warmup_steps^(-1.5))
```

### Cosine Annealing:
```
lr(t) = lr_min + (lr_max - lr_min) Â· (1 + cos(Ï€t/T)) / 2
```

---

## Formules Utiles

### Nombre de ParamÃ¨tres (Multi-Head Attention):
```
params_MHA = 4 Â· d_modelÂ² + 4 Â· d_model
          = 4d_model(d_model + 1)
```

### Nombre de ParamÃ¨tres (FFN):
```
params_FFN = 2 Â· d_model Â· d_ff + d_model + d_ff
```

### ParamÃ¨tres totaux par bloc:
```
params_block â‰ˆ 12d_modelÂ² + 13d_modelÂ·d_ff
```

Pour d_model=512, d_ff=2048:
```
params_block â‰ˆ 16.8M
```

---

**Note**: Ces formules sont simplifiÃ©es pour la clartÃ©. Les implÃ©mentations rÃ©elles peuvent avoir des variations mineures.
