# Build an LLM from Scratch - Part I: Attention & Transformer Core

## ğŸ“š Description du Workshop

Ce workshop vous guide dans la construction d'un Large Language Model (LLM) **from scratch** en utilisant PyTorch. Vous allez implÃ©menter et comprendre les mÃ©canismes fondamentaux qui alimentent des modÃ¨les comme GPT, BERT, et autres Transformers modernes.

## ğŸ¯ Objectifs d'apprentissage

1. âœ… **Self-Attention Mechanism** - Le cÅ“ur des Transformers
2. âœ… **Multi-Head Attention** - Apprentissage de patterns multiples
3. âœ… **Positional Encoding** - Encodage de la position des mots
4. âœ… **Transformer Encoder Block** - Architecture complÃ¨te
5. âœ… **Visualisation** - Comprendre ce que le modÃ¨le apprend

## ğŸ“¦ Structure des Notebooks

### 1ï¸âƒ£ `1_Self_Attention.ipynb`
**Comprendre le mÃ©canisme d'attention de base**
- ImplÃ©mentation du scaled dot-product attention
- Visualisation des poids d'attention
- Masque causal pour dÃ©codeurs
- Exemple concret avec des phrases

**Ce que vous allez apprendre:**
- Pourquoi l'attention est rÃ©volutionnaire
- Comment calculer Q, K, V
- L'importance du scaling par âˆšd_k
- InterprÃ©tation des heatmaps d'attention

### 2ï¸âƒ£ `2_Multi_Head_Attention.ipynb`
**Apprentissage de relations multiples en parallÃ¨le**
- ImplÃ©mentation du multi-head attention
- Comparaison 1 tÃªte vs multi-tÃªtes
- Visualisation des diffÃ©rentes tÃªtes
- Analyse de la complexitÃ©

**Ce que vous allez apprendre:**
- Pourquoi utiliser plusieurs tÃªtes
- Comment les tÃªtes capturent diffÃ©rents patterns
- Architecture et implÃ©mentation PyTorch
- Trade-offs en termes de paramÃ¨tres

### 3ï¸âƒ£ `3_Positional_Encoding.ipynb`
**Ajouter l'information de position**
- Encodage sinusoÃ¯dal (original Transformer)
- Alternatives (learned positional encoding)
- Visualisation des patterns
- PropriÃ©tÃ©s mathÃ©matiques

**Ce que vous allez apprendre:**
- Pourquoi les Transformers ont besoin de PE
- Comment fonctionnent les sinusoÃ¯des
- InterprÃ©tation des frÃ©quences
- Distance relative entre positions

### 4ï¸âƒ£ `4_Transformer_Encoder.ipynb`
**Assembler tous les composants**
- Layer Normalization
- Feed-Forward Networks
- Residual Connections
- Bloc Encoder complet
- Stack de N couches

**Ce que vous allez apprendre:**
- Architecture complÃ¨te du Transformer
- RÃ´le de chaque composant
- Comment empiler les couches
- Exemple d'application (classification)

### 5ï¸âƒ£ `5_Complete_Mini_LLM.ipynb` â­
**SynthÃ¨se finale - Un LLM fonctionnel!**
- Architecture complÃ¨te from scratch
- EntraÃ®nement sur toy dataset
- Visualisation des prÃ©dictions
- Attention heatmaps interprÃ©tables
- Code production-ready

**Ce que vous allez crÃ©er:**
- Un modÃ¨le complet de classification de sentiment
- Visualisations interactives
- Pipeline d'entraÃ®nement
- ModÃ¨le sauvegardable et rÃ©utilisable

## ğŸš€ Installation

```bash
# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install torch numpy matplotlib seaborn jupyter
```

## ğŸ“– Utilisation

### Option 1: Suivre l'ordre recommandÃ©
```bash
jupyter notebook
```

Puis ouvrez les notebooks dans l'ordre:
1. `1_Self_Attention.ipynb`
2. `2_Multi_Head_Attention.ipynb`
3. `3_Positional_Encoding.ipynb`
4. `4_Transformer_Encoder.ipynb`
5. `5_Complete_Mini_LLM.ipynb`

### Option 2: Aller directement au modÃ¨le complet
Si vous Ãªtes pressÃ©, ouvrez directement `5_Complete_Mini_LLM.ipynb` qui contient tout le code avec explications.

## ğŸ“ Concepts ClÃ©s ExpliquÃ©s

### Self-Attention
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```
- **Q (Query)**: Ce que je cherche
- **K (Key)**: Ce qui est disponible
- **V (Value)**: L'information rÃ©elle
- **Scaling**: Division par âˆšd_k pour stabilitÃ©

### Multi-Head Attention
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O
oÃ¹ head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)
```
- Permet d'apprendre plusieurs types de relations
- MÃªme complexitÃ© qu'une seule tÃªte
- ParallÃ©lisation efficace

### Positional Encoding
```
PE(pos, 2i) = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```
- Ajoute l'information de position
- FrÃ©quences variÃ©es pour diffÃ©rentes Ã©chelles
- Permet la gÃ©nÃ©ralisation Ã  des longueurs non vues

### Transformer Block
```
Block(x):
  1. x = x + MultiHeadAttention(x)
  2. x = LayerNorm(x)
  3. x = x + FeedForward(x)
  4. x = LayerNorm(x)
  return x
```

## ğŸ“Š RÃ©sultats Attendus

AprÃ¨s avoir complÃ©tÃ© les notebooks, vous aurez:

âœ… Un modÃ¨le Transformer fonctionnel  
âœ… ComprÃ©hension profonde de l'attention  
âœ… CapacitÃ© Ã  visualiser et interprÃ©ter les attention maps  
âœ… Code modulaire et rÃ©utilisable  
âœ… Base solide pour explorer des architectures plus complexes  

## ğŸ” Visualisations Incluses

- **Attention Heatmaps**: Voir quels mots le modÃ¨le regarde
- **Training Curves**: Loss et accuracy au fil du temps
- **Positional Encoding Patterns**: Visualiser les sinusoÃ¯des
- **Multi-head Comparisons**: Comparer diffÃ©rentes tÃªtes
- **Layer Analysis**: Ã‰volution des reprÃ©sentations

## ğŸ“š Pour Aller Plus Loin

### Papers Ã  lire:
1. **"Attention Is All You Need"** (Vaswani et al., 2017)
   - Le paper original du Transformer
   
2. **"BERT: Pre-training of Deep Bidirectional Transformers"** (Devlin et al., 2018)
   - Utilisation du Transformer pour le NLP
   
3. **"GPT-3: Language Models are Few-Shot Learners"** (Brown et al., 2020)
   - Scaling laws et Ã©mergence

### Ressources additionnelles:
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
- [Hugging Face Transformers Documentation](https://huggingface.co/docs/transformers/)

## ğŸ› ï¸ Customisation

Le code est modulaire et peut Ãªtre facilement adaptÃ© pour:
- DiffÃ©rentes tailles de modÃ¨les
- Autres tÃ¢ches (gÃ©nÃ©ration, traduction, etc.)
- Datasets personnalisÃ©s
- ExpÃ©rimentations architecturales

Exemple pour changer la configuration:
```python
model = MiniLLM(
    vocab_size=10000,
    d_model=512,      # â† Augmenter pour plus de capacitÃ©
    num_heads=8,      # â† Plus de tÃªtes = plus de patterns
    d_ff=2048,        # â† Dimension du FFN
    num_layers=6,     # â† Profondeur du modÃ¨le
    num_classes=3     # â† Nombre de classes Ã  prÃ©dire
)
```

## ğŸ’¡ Tips & Best Practices

### Pour l'entraÃ®nement:
- Commencer avec un petit modÃ¨le pour debugger
- Utiliser un learning rate scheduler
- Monitorer les gradients (gradient clipping si nÃ©cessaire)
- Sauvegarder les checkpoints rÃ©guliÃ¨rement

### Pour la visualisation:
- Examiner plusieurs exemples
- Comparer les patterns entre couches
- VÃ©rifier que l'attention a du sens sÃ©mantiquement

### Pour le debugging:
- VÃ©rifier les shapes Ã  chaque Ã©tape
- Tester avec des batch_size=1 d'abord
- Utiliser des assertions pour valider les dimensions

## ğŸ¤ Contribution

Ce workshop est conÃ§u pour Ãªtre Ã©ducatif. N'hÃ©sitez pas Ã :
- ExpÃ©rimenter avec le code
- Ajouter vos propres visualisations
- Tester sur vos donnÃ©es
- Partager vos dÃ©couvertes

## ğŸ“ Notes Importantes

âš ï¸ **Ce code est Ã  but Ã©ducatif**
- OptimisÃ© pour la clartÃ©, pas la performance
- Pour la production, utilisez des bibliothÃ¨ques comme Hugging Face
- Les toy datasets sont pour la dÃ©monstration

âœ¨ **Points forts du code:**
- Annotations dÃ©taillÃ©es en franÃ§ais
- Explications Ã©tape par Ã©tape
- Visualisations interactives
- Architecture modulaire

## ğŸ¯ Livrables du Workshop

Ã€ la fin de ce workshop, vous devez produire:

1. **Notebook fonctionnel** avec implÃ©mentation complÃ¨te âœ“
2. **Visualisations d'attention** montrant les patterns appris âœ“
3. **Courte explication** de vos rÃ©sultats (dÃ©jÃ  dans les notebooks) âœ“

## ğŸ“§ Support

Si vous avez des questions:
1. Consultez d'abord les commentaires dans le code
2. Regardez les cellules de visualisation
3. ExpÃ©rimentez avec diffÃ©rents paramÃ¨tres

## ğŸ‰ FÃ©licitations!

Vous avez maintenant les outils pour comprendre et construire des LLMs modernes!

**Next Steps:**
- Part II: Decoder Architecture & Text Generation
- Part III: Training at Scale
- Part IV: Fine-tuning & Applications

---

**Happy Learning! ğŸš€**

*Build, Learn, Iterate*
