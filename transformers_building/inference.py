import torch
from model.transformer import GPT
from tokenizer import CharTokenizer
from configurator import Config
import os

# ============================================================
# Charger la configuration
# ============================================================
cfg = Config("config.yaml")

# Charger le texte brut pour initialiser le tokenizer
with open(cfg.data_path, "r", encoding="utf-8") as f:
    text = f.read()

# Initialiser le tokenizer (char-level)
tokenizer = CharTokenizer(text)
vocab_size = tokenizer.vocab_size

# Choisir le device (CPU only si pas de GPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# ============================================================
# Charger le mod√®le GPT
# ============================================================
model = GPT(
    vocab_size=vocab_size,
    block_size=cfg.block_size,
    embed_dim=cfg.embed_dim,
    num_layers=cfg.num_layers,
    num_heads=cfg.num_heads,
    ff_hidden_dim=cfg.ff_hidden_dim,
    dropout=cfg.dropout
).to(device)

# ============================================================
# Charger le checkpoint sauvegard√©
# ============================================================
checkpoint_path = "checkpoints/ckpt.pt"
assert os.path.exists(checkpoint_path), "‚ùå ERREUR: checkpoint introuvable !"

checkpoint = torch.load(checkpoint_path, map_location=device)
model.load_state_dict(checkpoint["model"])
model.eval()

print("‚úÖ Mod√®le + checkpoint charg√© avec succ√®s")

# ============================================================
# Fonction de g√©n√©ration de texte
# ============================================================
@torch.no_grad()
def generate(model, start_tokens, max_new_tokens, tokenizer, device):
    model.eval()
    idx = torch.tensor(start_tokens, dtype=torch.long).unsqueeze(0).to(device)

    for _ in range(max_new_tokens):
        # Fen√™tre contextuelle limit√©e √† block_size
        idx_cond = idx[:, -cfg.block_size:]

        # üîß Le mod√®le retourne (logits, loss), donc on d√©balle le tuple
        output = model(idx_cond)
        if isinstance(output, tuple):
            logits = output[0]
        else:
            logits = output

        # On ne garde que les logits du dernier token
        logits = logits[:, -1, :]

        # On calcule les probabilit√©s avec softmax
        probs = torch.softmax(logits, dim=-1)

        # √âchantillonnage stochastique du prochain token
        idx_next = torch.multinomial(probs, num_samples=1)

        # On concat√®ne le token g√©n√©r√© √† la s√©quence
        idx = torch.cat((idx, idx_next), dim=1)

    # D√©codage en texte
    return tokenizer.decode(idx[0].tolist())

# ============================================================
# Exemple de g√©n√©ration
# ============================================================
prompt = "hello"
start_tokens = tokenizer.encode(prompt)

output = generate(
    model=model,
    start_tokens=start_tokens,
    max_new_tokens=100,   # longueur de texte √† g√©n√©rer
    tokenizer=tokenizer,
    device=device
)

print("\n===== OUTPUT =====")
print(output)
