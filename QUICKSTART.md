# ğŸš€ Guide de DÃ©marrage Rapide

## Installation (5 minutes)

```bash
# 1. CrÃ©er un environnement virtuel
python -m venv llm_env
source llm_env/bin/activate  # Linux/Mac
# ou: llm_env\Scripts\activate  # Windows

# 2. Installer les dÃ©pendances
pip install -r requirements.txt

# 3. Lancer Jupyter
jupyter notebook
```

## ğŸ“š Ordre RecommandÃ©

### Pour les dÃ©butants:
**Suivez l'ordre numÃ©rique** (environ 2-3 heures):

1. **1_Self_Attention.ipynb** (30 min)
   - Comprendre le mÃ©canisme de base
   - Visualiser les attention maps
   
2. **2_Multi_Head_Attention.ipynb** (30 min)
   - Apprendre le multi-head
   - Voir les diffÃ©rents patterns
   
3. **3_Positional_Encoding.ipynb** (20 min)
   - Comprendre l'encodage de position
   - Visualiser les sinusoÃ¯des
   
4. **4_Transformer_Encoder.ipynb** (40 min)
   - Assembler tous les composants
   - CrÃ©er un bloc complet
   
5. **5_Complete_Mini_LLM.ipynb** (60 min) â­
   - EntraÃ®ner un modÃ¨le complet
   - Visualiser les rÃ©sultats

### Pour les pressÃ©s:
**Allez directement au notebook 5** (1 heure):
- `5_Complete_Mini_LLM.ipynb` contient tout le code
- Explications complÃ¨tes incluses
- ModÃ¨le fonctionnel from scratch

## ğŸ¯ Ce que vous allez construire

Un mini-LLM capable de:
- âœ… Classifier des sentiments (positif/nÃ©gatif)
- âœ… Visualiser ses attention patterns
- âœ… ÃŠtre entraÃ®nÃ© sur vos donnÃ©es
- âœ… ÃŠtre Ã©tendu pour d'autres tÃ¢ches

## ğŸ“Š RÃ©sultats Attendus

AprÃ¨s ce workshop:
- âœ… ModÃ¨le fonctionnel crÃ©Ã© from scratch
- âœ… ComprÃ©hension profonde de l'attention
- âœ… Visualisations interprÃ©tables
- âœ… Base solide pour les LLMs avancÃ©s

## ğŸ’¡ Conseils

1. **ExÃ©cutez les cellules dans l'ordre**
2. **Lisez les commentaires** (trÃ¨s dÃ©taillÃ©s)
3. **ExpÃ©rimentez** avec les paramÃ¨tres
4. **Visualisez** Ã  chaque Ã©tape

## ğŸ› RÃ©solution de ProblÃ¨mes

### Erreur: "Module not found"
```bash
pip install --upgrade torch numpy matplotlib seaborn
```

### Jupyter ne dÃ©marre pas
```bash
pip install --upgrade jupyter ipykernel
python -m ipykernel install --user
```

### Erreur de mÃ©moire
RÃ©duisez les paramÃ¨tres du modÃ¨le:
```python
d_model = 64      # Au lieu de 512
num_layers = 2    # Au lieu de 6
```

## ğŸ“ Structure des Fichiers

```
.
â”œâ”€â”€ README.md                      # Documentation complÃ¨te
â”œâ”€â”€ QUICKSTART.md                  # Ce fichier
â”œâ”€â”€ requirements.txt               # DÃ©pendances
â”œâ”€â”€ 1_Self_Attention.ipynb        # Attention de base
â”œâ”€â”€ 2_Multi_Head_Attention.ipynb  # Multi-head
â”œâ”€â”€ 3_Positional_Encoding.ipynb   # Encodage position
â”œâ”€â”€ 4_Transformer_Encoder.ipynb   # Bloc encoder
â””â”€â”€ 5_Complete_Mini_LLM.ipynb     # ModÃ¨le complet â­
```

## ğŸ“ AprÃ¨s le Workshop

Vous serez capable de:
1. Comprendre les papers sur les Transformers
2. ImplÃ©menter vos propres variantes
3. Fine-tuner des modÃ¨les prÃ©-entraÃ®nÃ©s
4. Lire le code de GPT, BERT, etc.

## ğŸš€ Next Steps

AprÃ¨s avoir maÃ®trisÃ© ces notebooks:
- **Part II**: Decoder & gÃ©nÃ©ration de texte
- **Part III**: Training at scale
- **Part IV**: Fine-tuning & applications

## ğŸ“š Ressources SupplÃ©mentaires

- [Paper Original: "Attention Is All You Need"](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [Hugging Face Course](https://huggingface.co/course)

---

**PrÃªt Ã  commencer? Lancez Jupyter et ouvrez le premier notebook!** ğŸ‰

```bash
jupyter notebook 1_Self_Attention.ipynb
```
